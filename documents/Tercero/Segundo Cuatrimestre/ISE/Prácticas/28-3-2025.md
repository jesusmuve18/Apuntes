La tecnología de contenedores es originaria de Linux que se basaba en `namespace`. Te permitía que si estabas en un directorio de trabajo, por ejemplo `/home/ana/dev/ejercicio2 > namespace` (a lo mejor este no es exactamente el comando), en ese momento, el directorio de trabajo es `/>` (raíz del sistema de ficheros). Esto permite que el programa que vas a ejecutar a continuación no tiene acceso a todo el sistema de archivos, solo lo tiene desde el directorio que cuelga (ya que al ejecutar `cd ..` daría error al simular ser el directorio raíz). Si se tienen que instalar librerías y dependencias se instalarían en este directorio y permite borrarlas de forma segura. 

Si en este directorio ejecutamos `ps -ax` solo mostraría los procesos que cuelgan de este directorio (de forma que no se podrán comunicar con los que están fuera). Para poder comunicar dos namespaces tendremos que crear una red (esto supone una penalización de unos pocos milisegundos). La comunicación externa (por ejemplo internet) se haría en modo "bridge" (las tarjetas virtuales de los equipos virtuales son realmente las tarjetas del equipo real). Esto nos da un entorno de procesos, red y ejecución privado.

Supongamos que creamos una aplicación y le asociamos un filesystem dentro (directorio raíz, etc) con todas las dependencias que necesite dicha aplicación (en etapa de desarrollo). Cuando pasemos a fase de distribución nos garantizamos que funcionará, ya que es una solución autocontenida. Esto tiene el inconveniente de que si varias aplicaciones tienen dependencias similares se estarían duplicando. 

Por tanto los contenedores son mejores que las máquinas virtuales ya que no virtualizan y por ello tienen mejores prestaciones.

Hay que añadir el usuario al grupo `docker` para que pueda ejecutar contenedores (en caso contrario saldrá un error). En Linux, dockers se tendrá que inicializar con `systemctl`.

Lo primero que tendremos que hacer será instalar docker en el equipo. Podemos buscar contenedores en `hub.docker.com`. Los contenedores tienen una etiqueta que se suele utilizar para la versión del software. Hay distribuciones de linux específicas para los contenedores que tratan de ser lo más ligeras posibles (para que la aplicación ocupe lo menor posible). Por defecto, la etiqueta será `latest` ( que es un alias que por defecto selecciona la versión más reciente).

Podemos bajar el repositorio con `docker pull hello-world` y se puede ejecutar con `docker run hello-world`. Internamente, este repositorio funciona con Ubuntu. 

Para probar las prestaciones de un equipo se usan Benchmarks. Una suite es un conjunto de tests de propósito específico (por ejemplo ofimática). Los tests serían benchmarks individuales que normalmente son muy específicos y que tratar de probar un dispositivo (GPU, CPU). Podemos descargar benchmarks de `phoronix-test-suite.com`. Nosotros querremos ejecutarla dentro de su contenedor. Tiene sentido que se distribuyan así para que tengan todo lo que van a usar. Se puede descargar el benchmark de phoronix desde su perfil de github en "releases" (abajo en el panel de la derecha).

Como lo vamos a hacer como contenedores vamos a ejecutar `docker pull phoronix/pts`. Tendremos que ejecutarlo como `docker run -it phoronix/pts` para ejecutarlo en primer plano y con interfaz interactiva.

Si ejecutamos `system-info` nos dará la información sobre el sistema en el que está corriendo el contenedor (verá las características del ordenador físico). Podemos ver los sensores del sistema con `system-sensors` (los comandos están en la documentación de phoronix-test-suite).

Para ver los suits y los benchmark disponibles con `list-avalaiable-tests` y `list-avalaiable-suites`. Aparecerán clasificados por el objeto de lo que van a probar. Casi todos esos tests están en C y necesitarán instalar las dependencias. Para evitarlo podemos ejecutar `pts/compress-gzip` (que ya ha probado el profesor que no tiene). Podemos ver si cualquier benchmark tiene dependencias lo podemos hacer con `info pts/compress-7zip` y nos dirá las dependencias que tiene (que en este caso son de compilación).

Para descargar un benchmark podemos ejecutar `install pts/compress-gzip` o `benchmark pts/compress-gzip` (que lo instalará si no exite y sino lo ejecutará).

Apache Benchmark se utiliza para hacer benchmark de http. Ejecutamos `ab` (lo tenemos que instalar) y nos dará las opciones disponibles. Podemos hacer una prueba con `ab -n 10 -c 2 http://www.ugr.es` para ejecutar el 10 peticiones en 2 hebras al servidor de la ugr. Podremos ver que el Document Length no tiene mucho sentido (es un número demasiado pequeño). Para ver lo que está pasando podemos ejecutar `curl -v -X GET http://www.ugr.es` (con -v veremos información de las cabeceras) y podremos ver que nos devuelve un error que indique que la página se ha movido a https://www.ugr.es. Si ejecutamos `ab -n 10 -c 2 https://www.ugr.es` podremos ver que ya sí sale una cantidad razonable (aún así sigue sin incluir imágenes y recursos embebidos). Al ejecutar el comando aparecerá un informe con la siguiente información:
- Connect: tiempo que tardamos en llegar
- Processing: tiempo que tarda el servidor web en procesar la petición GET (hasta recibir el último byte)
- Waiting: Es parte de processing (desde que enviamos el GET hasta recibir el primer byte)

Apache benchmark se utiliza para hacer tests rápidos sobretodo para ver que las cosas funcionan (small tests). Esto se debe a que no hace todas las peticiones para cargar el recurso web y como benchmark no es muy adecuado para esto.

Jmeter (de apache) es una herramienta basada en java (habrá que instalarlo si no se tiene). Si ejecutamos `jmeter &` entraremos en una interfaz gráfica (bastante fea). Los tests plans empezarán con una raíz (Test Plan) y funcionan de igual forma que un git. Lo primero que creamos son los "Thread Groups" (botón derecho sobre "Test Plan" -> Thread -> Thread Groups). Estos se utilizan para modelar el comportamiento de distintos tipos de usuarios. Esto se utilizará para modelar el comportamiento esperado de cada perfil de la web. Podemos empezar creando un thread group con nombre "Estudiantes", Number of threads 1, ramp-up 1 (tiempo en el que se va incrementando la carga hasta llegar a la carga total esperada de threads, de forma lineal) y loop 1 (número de iteraciones que se van a realizar). Podemos irlo guardando para no perder los cambios. 

Podemos añadir un "Sampler" de HTTP Request con nombre "Home". En protocol podemos poner el protocolo que queramos (http por defecto) y en server Name or IP "www.ugr.es". En el request ponemos "GET" y path "/". 

Añadimos un listener para ver los resultados (View Results Tree). Es un listener muy pesado y por ello se suele desactivar (se puede desactivar sin eliminarlo). Podemos añadir un listener estadístico (Summary Report). Con esto tendremos una pequeñísima prueba de carga. Para ejecutarlo le damos al play. Para ver los resultados tendremos que irnos a los listeners que hemos añadido. En Advanced del HTTP request se puede activar el descargar todo el contenido de la página. Para borrar los resultados podemos hacer botón derecho "clear".

Para simular el comportamiento de un estudiante podremos crear otro HTTP Request con la dirección de la siguiente página a la que pensamos que podría acceder (por ejemplo en este caso https://www.ugr.es/estudiantes/grados). Podremos añadir timers para simular de forma más realista el tiempo que un usuario real tardaría en pasar de una página a otra.

Podemos crear otro perfil (thread group) que haga otro "recorrido" por la página web (otros enlaces, otro tiempo de espera, etc).

Podemos llevarnos los listeners al nivel superior para ver el resumen general.

Podemos añadir además un HTTP Request Defaults para añadir valores por defecto para un Test Plan. Podremos especificar una dirección ip, un protocolo, etc.

Dentro del Test Plan podemos definir variables como HOST donde podemos poner una dirección ip temporal y luego la podremos utilizar con `$[nombre_de_variable]`.

Para ejecutar la prueba no deberemos hacerlo desde la GUI. Podremos ejecutar el test como `jmeter -n -t PracticasISE.jmx -l resultados.jtl`.

El principal inconveniente de jmeter es usar un gestor de versiones ya que utiliza una tecnología muy desfasada.